---
title: "Model reproduction checklist"
author: "Breck Baldwin"
date: "3/1/2021"
output: 
  html_document:
    includes:
       in_header: _html/ga.html
---

# Model overview

This document exists in the repository at [https://github.com/codatmo/model_template](https://github.com/codatmo/model_template) as `index.Rmd` and rendered in html as `index.html`. The releases can be downloaded at [https://github.com/codatmo/model_template/releases](https://github.com/codatmo/model_template/releases) which includes this file. 

The goal of this model is to provide a template for reproducing COVID models in the CoDatMo framework. The model presented is a single parameter regression with artificial data. 

### Validation checklist items completed

* Model released on github
* Model description: Research goals, references, supplementary description as necessary.
* Data description and explicit data munging description.
* Small data set to validate model execution
* Divergent transitions report
* Rhat check
* Compare marginal posterior densities across chains
* Posterior predictive check
* Prior predictive check
* Simulated data parameter recovery
* Distribution created and released

### Validation steps planned

* SBC (Simulation Based Calibration)

## Problem description

This toy example attempts to model the relationship between phone calls to government services (111 calls) that report COVID-19 symptoms and hospital admissions that test positive for COVID-19 14 days later. 

## Model description
The model runs a simple linear regression with three parameters being estimated:

$$
y_n \sim \operatorname{normal}(\alpha + \beta X_n, \, \sigma) \\
\alpha \sim \operatorname{normal}(0,1) \\
\sigma \sim \operatorname{lognormal}(0,.5)
$$

## Data 

The data are artificially generated with generating values drawn randomly from priors. The generating script is at `generate_data.R`. Usually we don't have access to generating parameters so we won't consider them here. Later we will generate data as part of model validation.

## Data generating process

The data generating process is as follows: Person calls 111 reporting COVID symptoms as determined by the call center. We expect a baseline admission rate for COVID at hospitals and some percentage of the people who called will eventually be admitted to the hospital 14 days later. 
```{r}
library(dagitty)
graph <- dagitty("dag {x_calls_111 -> y_cvd_hosp}")
coordinates(graph) <- list(x=c(x_calls_111=1, y_cvd_hosp=2),
                           y=c(x_calls_111=0, y_cvd_hosp=0))
plot(graph)
```
The above causal graph treats 111 calls as causal for hospital admissions which is certainly incorrect. A more complete causal model would be sensitive to the underlying COVID rate but we are keeping the model simple. 

Below the data are loaded.

```{r}
source(file="data/data.R")
# vars are: 'n_days_data','x_calls_111', 'y_cvd_hosp'
data <- data.frame(x_calls_111, y_cvd_hosp)

head(data)
```

### Data munging and examination
There is no data cleaning or processing. We will graph it however.

```{r}
library(ggplot2)
ggplot(data) + aes(x=x_calls_111,y=y_cvd_hosp) + geom_point()
```
The conversion to Stan input given the above data is as follows:
```{r echo=TRUE, message=FALSE, comment=NA}
stan_data <- list(N=nrow(data), x_calls_111=data$x_calls_111, y_cvd_hosp=data$y_cvd_hosp,
                  compute_likelihood=1, compute_prediction=1)
```

## Stan program

The Stan model is located at `model_template/stan/linear_regression.stan`:

```{r echo=FALSE, message=FALSE, comment=NA}
stan_file <- "stan/linear_regression.stan"
lines <- readLines(stan_file)
cat(paste(lines, "\n", sep=""), sep="")

get_data_block <- function(lines) {
  data_lines <- c()
  accumulate_line <- FALSE
  for (line in lines) {
    if (str_detect(line,"^data\\s*\\{")) {
      accumulate_line <- TRUE
    }
    if (str_detect(line,"^parameters\\s*\\{")) {
      accumulate_line <- FALSE
      break
    }
    if (accumulate_line) {
      data_lines <- c(data_lines,line)
    }
  }
  return(data_lines)
}
```
## Running model

```{r echo=TRUE, message=FALSE, comment=NA}
library(cmdstanr)
model <- cmdstan_model(file.path("stan","linear_regression.stan"))
stan_data <- list(N=nrow(data), x_calls_111=data$x_calls_111, y_cvd_hosp=data$y_cvd_hosp,
                  compute_likelihood=1, compute_prediction=0)
fit <- model$sample(data=stan_data, seed=999, chains=4)
```

Viewing a text based summary of the fit:

```{r}
fit$cmdstan_summary()
```
Viewing the posteriors graphically yeilds:
```{r}
library(bayesplot)
mcmc_hist(fit$draws(variables=c("sigma_sd", "beta_coef_111_call", "alpha_intercept")))
```


## Model validation

Below are the diagnostics used to help validate the model. 

### Run posterior diagnostics

There are standard diagnostics that look for errors in the posterior.

```{r}
fit$cmdstan_diagnose()
```

* Treedepth warnings passed
* Divergence check passed
* E-BFMI satisfactory
* R-hat values satisfactory

### Prior predictive check

The prior predictive check estimates the model parameters without the likelihood being used. The resulting draws are then used to predict new data via predictive application of the likelihood given the draws $$ p(ysim|\alpha,\beta,\sigma) $$. Note that `compute_likelihood = 0` prevents the likelihood being computed in the model. 

```{r echo=TRUE, comment=NA}
library(cmdstanr)
library(rstan)
library(ggplot2)
library(bayesplot)
library(tidyr)

model <- cmdstan_model(file.path("stan","linear_regression.stan"))
stan_data <- list(N=100, x_calls_111=data$x_calls_111, y_cvd_hosp=y_cvd_hosp,
                  compute_likelihood=0, compute_prediction=1)

fit <- model$sample(data=stan_data, seed=999, chains=4)

mcmc_hist(fit$draws(variables=c("sigma_sd", "beta_coef_111_call", "alpha_intercept")))
```
Above we see the posterior of the priors without seeing any data. Generally some justification for prior distributions is expected. The above are weakly informative in that they cover a broad range of plausible values. 

#### Plotting simulations from priors

Draws from the above posteriors allow for generation of simulated output data given input data `x_calls_111`. Note that `compute_likelihood=0` prevents adding information from the data. 

```{r}
rs_fit <- rstan::read_stan_csv(fit$output_files())
rs_ex <- rstan::extract(rs_fit)
random_draws <- sample(1:nrow(rs_ex$y_cvd_hosp_pred), 10, replace=FALSE)
draws <- data.frame(t(rs_ex$y_cvd_hosp_pred[random_draws,]))
names(draws) <- random_draws
draw_names <- colnames(draws)

p_data2 <- cbind(data,draws)

p_long_data <- gather(p_data2,draw,y_sim,draw_names)

p <- ggplot(data=p_long_data, aes(x=x_calls_111)) +
            geom_point(aes(y=y_sim, group=draw, color=draw), size=.5) +
            geom_line(aes(y=y_cvd_hosp), color="black", size=.5)
print(p)

```
Actual data is plotted as a black line for context. At this point some comforting statements are made that the prior's informativeness is minimal and that Bayes himself would bless the entire effort were he alive to do so. 

### Parameter recovery with simulated data

Parameter recovery establishes that for some small set of values the model reasons properly. $$ p(\alpha',\beta',\sigma'|y\_sim) $$ We pick a draw from the above distributions, simulate data with it and then attempt to recover the parameters that we simulated with. We can look at the above graph and pick expected outliers or close to actual data. For this we pick 2959 as a middle-of-the-road example. 

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
#Pick one arbitrary draw from the prior distribution
draw <- 2959
stan_data <- list(N=100, 
                  x_calls_111=data$x_calls_111,
                  y_cvd_hosp=rs_ex$y_cvd_hosp_pred[draw,],
                  compute_likelihood=1, compute_prediction=0)
fit <- model$sample(data=stan_data, seed=999, chains=4)

fit$cmdstan_summary()
cat(sprintf("\nDraw number %d",draw))
cat(sprintf("\nactual alpha_intercept=%.2f",rs_ex$alpha_intercept[draw]))
cat(sprintf("\nactual beta_coef_111_call=%.2f",rs_ex$beta_coef_111_call[draw]))
cat(sprintf("\nactual sigma_sd=%.2f",rs_ex$sigma_sd[draw]))

```
All the parameters fit within the 2.5% to to 97.5% interval.

### Posterior predictive check

Like the prior predictive check but we include actual data in estimating the parameters. Note that both `compute_liklihood=1`,  `compute_prediction=1` and that the actual data is supplied in place of simlated data from above.

 
```{r}
stan_data <- list(N=100, x_calls_111=data$x_calls_111, y_cvd_hosp=y_cvd_hosp,
                  compute_likelihood=1, compute_prediction=1)

fit <- model$sample(data=stan_data, seed=999, chains=4)
rs_fit <- rstan::read_stan_csv(fit$output_files())
rs_ex <- rstan::extract(rs_fit)
random_draws <- sample(1:nrow(rs_ex$y_cvd_hosp_pred), 10, replace=FALSE)
draws <- data.frame(t(rs_ex$y_cvd_hosp_pred[random_draws,]))
names(draws) <- random_draws
draw_names <- colnames(draws)

p_data2 <- cbind(data,draws)

p_long_data <- gather(p_data2,draw,y_sim,draw_names)

p <- ggplot(data=p_long_data, aes(x=x_calls_111)) +
            geom_point(aes(y=y_sim, group=draw, color=draw), size=.5) +
            geom_line(aes(y=y_cvd_hosp), color="black", size=.5)
print(p)
```
Actual data as a black line, 10 draws from the posterior shown. Some comments on how to read these tea leaves.  

### Cross validation





### SBC validation

20 prior theta, 100 draws from each posterior, for each param, it should be uniform in it's rank

.5 -> y, y -> theta_hat 100 draws

